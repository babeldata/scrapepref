# Scraper des arrÃªtÃ©s de la prÃ©fecture de police de Paris

Scraper automatisÃ© pour extraire les arrÃªtÃ©s publiÃ©s sur le site de la prÃ©fecture de police de Paris, avec classification automatique des arrÃªtÃ©s de circulation.

## ğŸ¯ FonctionnalitÃ©s

- âœ… Extraction automatique des arrÃªtÃ©s depuis le site de la prÃ©fecture de police
- âœ… Classification automatique des arrÃªtÃ©s de circulation (recherche du mot "circulation" dans le titre)
- âœ… Nettoyage automatique des donnÃ©es (sÃ©paration des arrÃªtÃ©s concatÃ©nÃ©s)
- âœ… Extraction prÃ©cise du numÃ©ro d'arrÃªtÃ©, titre et date
- âœ… TÃ©lÃ©chargement des PDFs
- âœ… Upload vers S3 (AWS S3 ou MinIO)
- âœ… Export CSV avec mÃ©tadonnÃ©es complÃ¨tes
- âœ… CSV sÃ©parÃ© pour les arrÃªtÃ©s de circulation
- âœ… Automatisation via GitHub Actions
- âœ… Mode test (DRY_RUN) sans upload S3
- âœ… Support Firefox en fallback si Chromium ne fonctionne pas

## ğŸ“‹ PrÃ©requis

- **Python 3.11+**
- **uv** : Gestionnaire de paquets ultra-rapide (recommandÃ©)
- **Playwright** : Navigateur headless pour JavaScript
- **Compte AWS S3** (ou MinIO) pour stocker les PDFs

## ğŸš€ Installation

### 1. Cloner le repository

```bash
git clone https://github.com/babeldata/scrapepref.git
cd scrapepref
```

### 2. Installer les dÃ©pendances

**Avec uv (recommandÃ©)** :

```bash
# Installer uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# Installer les dÃ©pendances Python
uv pip install -r requirements.txt

# Installer Playwright
playwright install chromium
```

**Avec pip classique** :

```bash
pip install -r requirements.txt
playwright install chromium
```

### 3. Configurer les variables d'environnement

Copiez `env.example` vers `.env` et configurez :

```bash
cp env.example .env
```

Ã‰ditez `.env` avec vos credentials :

```env
# Configuration AWS S3
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here
AWS_REGION=us-east-1
S3_BUCKET_NAME=your_bucket_name_here

# Pour MinIO ou autre S3-compatible: spÃ©cifier l'URL
# Exemples: http://localhost:9000 ou https://minio.example.com
# Laisser vide pour AWS S3 standard
S3_ENDPOINT_URL=https://minio.example.com

# Configuration du scraper
SCRAPE_DELAY_SECONDS=2
MAX_CONCURRENT_PAGES=5
MAX_PAGES_TO_SCRAPE=0  # 0 = toutes les pages

# Mode test (sans upload S3)
DRY_RUN=false
```

### 4. Configurer GitHub Secrets (pour l'automatisation)

**Option A - Secrets au niveau du repository (recommandÃ© pour dÃ©buter)** :

Dans votre repository GitHub, aller dans `Settings > Secrets and variables > Actions > Repository secrets` et ajouter :

* `AWS_ACCESS_KEY_ID`
* `AWS_SECRET_ACCESS_KEY`
* `AWS_REGION`
* `S3_BUCKET_NAME`
* `S3_ENDPOINT_URL` (laisser vide pour AWS S3, ou votre URL MinIO, ex: `https://minio.example.com`)

**Option B - CrÃ©er un Environment (recommandÃ© pour la production)** :

1. Dans votre repository : `Settings > Environments > New environment`
2. Nommez-le `production`
3. Ajoutez les mÃªmes 5 secrets dans cet environment
4. Dans `.github/workflows/daily_scrape.yml`, dÃ©commentez la ligne `# environment: production`
5. (Optionnel) Configurez des protections : approbation manuelle, restrictions de branches, etc.

**Note pour MinIO** : Le scraper supporte nativement MinIO et autres services compatibles S3. Il suffit de spÃ©cifier votre endpoint dans `S3_ENDPOINT_URL`.

## ğŸ§ª Test Local

Pour tester rapidement le scraper sur votre machine :

```bash
# En mode DRY_RUN (sans upload S3 - pas besoin de credentials)
export DRY_RUN=true
export MAX_PAGES_TO_SCRAPE=2  # Limiter Ã  2 pages
python run_local.py
```

Le script `run_local.py` :

* âœ… VÃ©rifie la configuration
* âœ… Limite automatiquement Ã  2 pages en mode test
* âœ… Affiche des messages d'aide clairs
* âœ… Sauvegarde le HTML dans `data/debug_page_*.html` pour debug
* âœ… GÃ©nÃ¨re le CSV avec les mÃ©tadonnÃ©es

**Script de debug HTML** :

Pour analyser la structure HTML du site en dÃ©tail :

```bash
python test_local.py
```

Ce script :

* RÃ©cupÃ¨re la premiÃ¨re page de rÃ©sultats
* Sauvegarde le HTML dans `debug_local.html`
* Analyse les Ã©lÃ©ments HTML et recherche les patterns
* Affiche un rapport dÃ©taillÃ© dans la console

## ğŸ’» Utilisation

### ExÃ©cution manuelle

```bash
cd src
python scraper.py
```

### Mode test (DRY_RUN)

Pour tester le scraper sans uploader vers S3 :

```bash
export DRY_RUN=true
export MAX_PAGES_TO_SCRAPE=1  # Limiter Ã  1 page pour les tests
cd src
python scraper.py
```

Le mode DRY_RUN :

* Ne nÃ©cessite pas de credentials S3
* Simule l'upload des PDFs
* Enregistre quand mÃªme les mÃ©tadonnÃ©es dans le CSV
* Affiche `[DRY_RUN]` dans les logs

### ExÃ©cution automatique

Le GitHub Action s'exÃ©cute automatiquement tous les jours Ã  6h du matin (heure de Paris).

### Test avec GitHub Actions (mode dry-run)

Pour tester le scraper sans uploader vers S3 :

1. Allez dans l'onglet **Actions** de votre repo GitHub
2. SÃ©lectionnez **"Test Scraper (Dry Run)"** dans la liste des workflows
3. Cliquez sur **"Run workflow"**
4. Configurez les paramÃ¨tres :  
   * **max_pages** : `1` (nombre de pages Ã  scraper)  
   * **dry_run** : `true` (pas d'upload S3 rÃ©el)  
   * **max_concurrent** : `3` (pages en parallÃ¨le)
5. Cliquez sur **"Run workflow"** (bouton vert)

Le workflow va :

* âœ… Scraper 1 page de rÃ©sultats
* âœ… Simuler l'upload des PDFs (pas d'upload rÃ©el)
* âœ… Afficher un rÃ©sumÃ© dans l'interface GitHub
* âœ… Uploader les logs et le CSV comme artefacts (tÃ©lÃ©chargeables pendant 7 jours)

### Lancement manuel du scraping complet

Vous pouvez aussi lancer manuellement le scraping complet depuis l'interface GitHub :

1. Aller dans l'onglet `Actions`
2. SÃ©lectionner `Daily Scrape of Prefecture ArrÃªtÃ©s`
3. Cliquer sur `Run workflow`

### Rescraping des PDFs manquants (mise Ã  jour des URLs S3)

Si certains arrÃªtÃ©s dans le CSV n'ont pas de lien S3, vous pouvez utiliser le workflow de rescraping pour tÃ©lÃ©charger et uploader les PDFs manquants :

**Depuis GitHub Actions** :

1. Aller dans l'onglet `Actions`
2. SÃ©lectionner `Rescrape Missing S3 URLs`
3. Cliquer sur `Run workflow`
4. Configurer les paramÃ¨tres :
   * **dry_run** : `false` (pour uploader rÃ©ellement) ou `true` (pour tester)
   * **scrape_delay** : `2` (dÃ©lai en secondes entre tÃ©lÃ©chargements)
5. Cliquer sur `Run workflow`

Le workflow va :
* âœ… Identifier les lignes du CSV `arretes_circulation.csv` sans `pdf_s3_url`
* âœ… VÃ©rifier si les fichiers existent dÃ©jÃ  sur S3
* âœ… TÃ©lÃ©charger les PDFs manquants depuis les URLs originales
* âœ… Uploader les PDFs sur S3
* âœ… Mettre Ã  jour le CSV avec les nouvelles URLs S3
* âœ… Commiter automatiquement les changements

**En local** :

```bash
python rescrape_missing_s3.py
```

Le script utilise les mÃªmes variables d'environnement que le scraper principal.

## ğŸ“ Structure des donnÃ©es

### CSV (`data/arretes.csv`)

Colonnes :

* `numero_arrete` : NumÃ©ro unique de l'arrÃªtÃ© (format : 2025-01535)
* `titre` : Titre complet de l'arrÃªtÃ© (nettoyÃ©, sans concatÃ©nation avec d'autres arrÃªtÃ©s)
* `date_publication` : Date de publication (format DD/MM/YYYY)
* `lien` : URL de la page de l'arrÃªtÃ©
* `pdf_url` : URL du PDF
* `is_circulation` : `True` si c'est un arrÃªtÃ© de circulation (contient "circulation" dans le titre), `False` sinon
* `contenu_preview` : AperÃ§u du contenu (200 premiers caractÃ¨res, nettoyÃ©)
* `pdf_s3_url` : URL S3 du PDF (`s3://bucket/arretes/2025/arrete_abc12345.pdf`)
* `poids_pdf_ko` : Taille du PDF en Ko
* `date_scrape` : Date et heure du scraping (ISO 8601)

### CSV Circulation (`data/arretes_circulation.csv`)

Fichier CSV sÃ©parÃ© contenant uniquement les arrÃªtÃ©s de circulation (oÃ¹ `is_circulation == True`).

### S3

Les PDFs sont organisÃ©s par annÃ©e :

```
s3://your-bucket/arretes/
â”œâ”€â”€ 2025/
â”‚   â”œâ”€â”€ arrete_abc12345.pdf
â”‚   â”œâ”€â”€ arrete_def67890.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 2024/
â”‚   â””â”€â”€ ...
```

Le hash MD5 (8 premiers caractÃ¨res) est ajoutÃ© au nom de fichier pour Ã©viter les duplicatas.

## ğŸ” Classification des arrÃªtÃ©s de circulation

Le scraper classe automatiquement les arrÃªtÃ©s selon qu'ils concernent la circulation ou non. 

**MÃ©thode simple et efficace** : La classification se base uniquement sur la prÃ©sence du mot **"circulation"** dans le titre de l'arrÃªtÃ© (insensible Ã  la casse).

Un arrÃªtÃ© est classÃ© comme arrÃªtÃ© de circulation si son titre contient le mot "circulation". Cette mÃ©thode simple garantit une prÃ©cision Ã©levÃ©e et Ã©vite les faux positifs.

## âš™ï¸ Configuration avancÃ©e

### Limiter le scraping

Pour tester ou limiter le nombre de pages scrapÃ©es :

```bash
export MAX_PAGES_TO_SCRAPE=5  # Scraper seulement les 5 premiÃ¨res pages
python src/scraper.py
```

### Ajuster la vitesse

Le site peut Ãªtre lent. Les dÃ©lais par dÃ©faut sont :

* `SCRAPE_DELAY_SECONDS=2` : DÃ©lai entre chaque requÃªte
* `MAX_CONCURRENT_PAGES=5` : Nombre de pages ouvertes en parallÃ¨le

Vous pouvez augmenter ces valeurs si vous rencontrez des timeouts.

### Ajuster les timeouts

Si le site est trÃ¨s lent ou que vous rencontrez des timeouts, augmentez ces valeurs (en millisecondes) :

```bash
export PAGE_LOAD_TIMEOUT=120000  # 120 secondes pour charger une page (dÃ©faut: 90s)
export PDF_DOWNLOAD_TIMEOUT=90000  # 90 secondes pour tÃ©lÃ©charger un PDF (dÃ©faut: 60s)
```

Ces timeouts contrÃ´lent combien de temps Playwright attend avant d'abandonner une opÃ©ration.

### Personnaliser la classification

Pour modifier les critÃ¨res de classification des arrÃªtÃ©s de circulation, Ã©ditez la fonction `is_circulation_arrete()` dans `src/scraper.py` :

```python
def is_circulation_arrete(titre: str, contenu: str = "") -> bool:
    """
    DÃ©termine si un arrÃªtÃ© concerne la circulation.
    Simple : cherche le mot "circulation" dans le titre.
    """
    titre_lower = titre.lower()
    return 'circulation' in titre_lower
```

Vous pouvez modifier cette fonction pour ajouter d'autres critÃ¨res si nÃ©cessaire (par exemple, chercher aussi "stationnement" ou d'autres mots-clÃ©s).

## ğŸ“Š Statistiques

Le scraper affiche des statistiques Ã  la fin de l'exÃ©cution :

```
Statistiques:
  Total arrÃªtÃ©s: 150
  ArrÃªtÃ©s de circulation: 45
  Autres arrÃªtÃ©s: 105
```

**Note sur la classification** : Un arrÃªtÃ© est classÃ© comme "circulation" si son titre contient le mot "circulation". Cette mÃ©thode simple garantit une prÃ©cision Ã©levÃ©e.

## ğŸ› ProblÃ¨mes connus

1. **Site lent** : Le site peut Ãªtre trÃ¨s lent. Les timeouts sont configurÃ©s Ã  90 secondes par dÃ©faut.
2. **TÃ©lÃ©chargement PDF** : Certains PDFs peuvent Ãªtre inaccessibles (document retirÃ©, erreur serveur). Dans ce cas, le scraper enregistre `ERROR: PDF non tÃ©lÃ©chargÃ©` dans le CSV.
3. **Rate limiting** : Si trop de requÃªtes sont faites rapidement, le site peut bloquer temporairement. Ajustez `SCRAPE_DELAY_SECONDS`.
4. **Structure HTML** : La structure HTML du site peut changer. Utilisez `test_local.py` pour analyser la structure actuelle et adapter les sÃ©lecteurs dans `scraper.py`.
5. **Chromium sur macOS** : Si Chromium crash, le scraper essaie automatiquement Firefox en fallback.

## ğŸ”§ DÃ©pendances

* **Python 3.11+**
* **uv** : Gestionnaire de paquets ultra-rapide (recommandÃ©)
* **Playwright** : Navigateur headless pour JavaScript
* **BeautifulSoup4** : Parsing HTML
* **Pandas** : Gestion CSV
* **Boto3** : Upload S3
* **python-dotenv** : Variables d'environnement
* **lxml** : Parser XML/HTML rapide

## ğŸ“ Licence

Ce projet est sous licence MIT.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou une pull request.

## âš ï¸ Avertissement

Ce scraper est conÃ§u pour un usage Ã©ducatif et de recherche. Assurez-vous de respecter les conditions d'utilisation du site de la prÃ©fecture de police et les lois en vigueur concernant le scraping de donnÃ©es publiques.

## ğŸ“š Ressources

- [Site de la prÃ©fecture de police](https://www.prefecturedepolice.interieur.gouv.fr/actualites-et-presse/arretes/accueil-arretes)
- [Documentation Playwright](https://playwright.dev/python/)
- [Documentation Boto3 (S3)](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)

